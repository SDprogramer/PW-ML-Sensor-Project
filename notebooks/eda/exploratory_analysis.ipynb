{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c09986b3",
   "metadata": {},
   "source": [
    "#  Wafer fault Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c01b4d6",
   "metadata": {},
   "source": [
    "**Brief:** In electronics, a **wafer** (also called a slice or substrate) is a thin slice of semiconductor, such as a crystalline silicon (c-Si), used for the fabrication of integrated circuits and, in photovoltaics, to manufacture solar cells. The wafer serves as the substrate(serves as foundation for contruction of other components) for microelectronic devices built in and upon the wafer. \n",
    "\n",
    "It undergoes many microfabrication processes, such as doping, ion implantation, etching, thin-film deposition of various materials, and photolithographic patterning. Finally, the individual microcircuits are separated by wafer dicing and packaged as an integrated circuit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf9bbf5e",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "**Data:** Wafers data\n",
    "\n",
    "\n",
    "**Problem Statement:** Wafers are predominantly used to manufacture solar cells and are located at remote locations in bulk and they themselves consist of few hundreds of sensors. Wafers are fundamental of photovoltaic power generation, and production thereof requires high technology. Photovoltaic power generation system converts sunlight energy directly to electrical energy.\n",
    "\n",
    "The motto behind figuring out the faulty wafers is to obliterate the need of having manual man-power doing the same. And make no mistake when we're saying this, even when they suspect a certain wafer to be faulty, they had to open the wafer from the scratch and deal with the issue, and by doing so all the wafers in the vicinity had to be stopped disrupting the whole process and stuff anf this is when that certain wafer was indeed faulty, however, when their suspicion came outta be false negative, then we can only imagine the waste of time, man-power and ofcourse, cost incurred.\n",
    "\n",
    "**Solution:** Data fetched by wafers is to be passed through the machine learning pipeline and it is to be determined whether the wafer at hand is faulty or not apparently obliterating the need and thus cost of hiring manual labour."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd0b151e",
   "metadata": {},
   "source": [
    "## # Import Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed7700",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.8.0)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p c:\\ZZZ_PROGRAMING\\DS ML Gen AI\\ML Sensor Project\\venv ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c36938",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the feature store dataset as dataframe\n",
    "\n",
    "file_path = r\"C:\\ZZZ_PROGRAMING\\DS ML Gen AI\\ML Sensor Project\\notebooks\\wafer_23012020_041211.csv\"\n",
    "wafers = pd.read_csv(file_path)\n",
    "print(\"Shape of the feature store dataset: \", wafers.shape)\n",
    "wafers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.columns\n",
    "## 592 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec7bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6668d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.drop(columns = [\"Unnamed: 0\", \"Good/Bad\"]).iloc[ : 100].to_csv(\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102fd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the column unnamed: 0 as wafer\n",
    "\n",
    "wafers.rename(columns = {\"Unnamed: 0\" : \"Wafer\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ece4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train-Test Split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wafers, wafers_test = train_test_split(wafers, test_size = .20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed57077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Wafers' Info\n",
    "\n",
    "wafers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63000ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Description of `wafers`\n",
    "\n",
    "wafers.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a032aae3",
   "metadata": {},
   "source": [
    "### Insight:\n",
    "\n",
    "From the gist of only shown columns, it looks like some of features have pretty bad outliers. One thing is for sure, the data must be standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1804bdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at the Cats in our Target feature\n",
    "wafers['Good/Bad'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab64f53e",
   "metadata": {},
   "source": [
    "### Insight:\n",
    "\n",
    "Heavily imbalanced. Definitely gonna need `resampling`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a7bd9d4",
   "metadata": {},
   "source": [
    "## # Analyze Missing Data:\n",
    "\n",
    "Firstly, we'll check the missing data in the target feature and drop those records. **As if we already know a value of target feature then there's no need for a ML algorithm, damn right?** Therefore, the best way to deal with missing target entries is to delete them. For other missing features, we can definitely use impute strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5684b58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check missing values in target feature\n",
    "wafers[\"Good/Bad\"].isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9abe6ca4",
   "metadata": {},
   "source": [
    "**=>** Woa, not even a single missing entry, I didn't see that coming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fcd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11109727",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check missing values in dependent feature variables\n",
    "## Chnaging into percentage\n",
    "wafers.isna().sum().sum() / (wafers.shape[0] * (wafers.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa31a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb15b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wafers.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3b3e480",
   "metadata": {},
   "source": [
    "**=>** Almost 4% out of total cells we're having, are missing.\n",
    "\n",
    "We're gonna try all sort of imputation strategies and would choose the one with that's gonna give us least overall-error-val."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a39b74a",
   "metadata": {},
   "source": [
    "## # Visualization of Sensors' distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the distribution first 50 sensors of Wafers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 50 random sensors\n",
    "random_50_sensors_idx = []\n",
    "for i in range(50):\n",
    "    if i not in random_50_sensors_idx:\n",
    "        random_50_sensors_idx.append(np.random.randint(1, 591))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10faa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now, have a look at the distribution of random 50 sensors\n",
    "plt.figure(figsize = (15, 100))\n",
    "for i, col in enumerate(wafers.columns[random_50_sensors_idx]):\n",
    "    plt.subplot(60, 3, i + 1)\n",
    "    sns.distplot(x = wafers[col], color = 'indianred')\n",
    "    plt.xlabel(col, weight = 'bold')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ff116ee",
   "metadata": {},
   "source": [
    "### Insight:\n",
    "\n",
    "Pretty good amount of them (either first 50 or random 50) either are constant (have 0 standard deviation) or have left skewness and right skewness. It ain't possible to analyze each feature and deal with its outliers individually, thus we oughta depend upon the scaling. \n",
    "\n",
    "For the **features with 0 standard deviation**, we can straight away drop them and for others that do have outliers, we gotta go ahead with the `Robust Scaling`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69515484",
   "metadata": {},
   "source": [
    "### # Get Columns to Drop:\n",
    "\n",
    "Will drop columns with zero standard deviation as they are not influencing the target variable in any way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be449cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_with_zero_std_dev(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Returns a list of columns names who are having zero standard deviation.\n",
    "    \"\"\"\n",
    "    cols_to_drop = []\n",
    "    num_cols = [col for col in df.columns if df[col].dtype != 'O']  # numerical cols only\n",
    "    for col in num_cols:\n",
    "        if df[col].std() == 0:\n",
    "            cols_to_drop.append(col)\n",
    "    return cols_to_drop\n",
    "\n",
    "def get_redundant_cols(df: pd.DataFrame, missing_thresh=.7):\n",
    "    \"\"\"\n",
    "    Returns a list of columns having missing values more than certain thresh.\n",
    "    \"\"\"\n",
    "    cols_missing_ratios = df.isna().sum().div(df.shape[0])\n",
    "    cols_to_drop = list(cols_missing_ratios[cols_missing_ratios > missing_thresh].index)\n",
    "    return cols_to_drop        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d3e38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns w missing vals more than 70%\n",
    "cols_to_drop_1 = get_redundant_cols(wafers, missing_thresh=.7)\n",
    "cols_to_drop_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ea73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Columns w 0 Standard Deviation\n",
    "cols_to_drop_2 = get_cols_with_zero_std_dev(df = wafers)\n",
    "cols_to_drop_2.append(\"Wafer\")\n",
    "cols_to_drop_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cols to drop\n",
    "cols_to_drop = cols_to_drop_1 + cols_to_drop_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7269a6c9",
   "metadata": {},
   "source": [
    "**=>** Features that are not gonna contribute to ML algorithm in anyway, whatsoever."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa4c8647",
   "metadata": {},
   "source": [
    "## # Separate Features and Labels out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0240f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate features and Labels out\n",
    "X, y = wafers.drop(cols_to_drop, axis = 1), wafers[[\"Good/Bad\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependent feature variables\n",
    "print(\"Shape of the features now: \", X.shape)\n",
    "X.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c02176c",
   "metadata": {},
   "source": [
    "**=>** Now, we have 475 contributing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cc448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Independent/Target Variables\n",
    "print(\"Shape of the labels: \", y.shape)\n",
    "y.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fccb956d",
   "metadata": {},
   "source": [
    "## # Data Transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841c7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "imputer = KNNImputer(n_neighbors = 3)\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    steps = [('Imputer', imputer), ('Scaler', RobustScaler())])\n",
    "preprocessing_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046cdf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform \"Wafers\" features\n",
    "X_trans = preprocessing_pipeline.fit_transform(X)\n",
    "print(\"Shape of transformed features set: \", X_trans.shape)\n",
    "X_trans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663ef07d",
   "metadata": {},
   "source": [
    "## # Shall we cluster \"Wafers\" instances?\n",
    "\n",
    "Let's see whether clustering of data instances do us any good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0760614",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df8520f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ClusterDataInstances:\n",
    "    \"\"\"Divides the given data instances into different clusters via KMeans Clustering algorithm.\n",
    "    Args:\n",
    "        X (np.array): Takes in an array which gotta be clustered.\n",
    "        desc (str): Description of the said array.\n",
    "    \"\"\"\n",
    "    X: np.array\n",
    "    desc: str\n",
    "\n",
    "    def _get_ideal_number_of_clusters(self):\n",
    "        \"\"\"Returns the ideal number of clusters the given data instances should be divided into by \n",
    "        locating the dispersal point in number of clusters vs WCSS plot.\n",
    "\n",
    "        Raises:\n",
    "            e: Raises relevant exception should any kinda error pops up while determining the ideal\n",
    "            number of clusters.\n",
    "\n",
    "        Returns:\n",
    "            int: Ideal number of clusters the given data instances should be divided into.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(\n",
    "                f'Getting the ideal number of clusters to cluster \"{self.desc} set\" into..')\n",
    "            ####################### Compute WCSS for shortlisted number of clusters ##########################\n",
    "            print(\"computing WCSS for shortlisted number of clusters..\")\n",
    "            wcss = []  # Within Summation of Squares\n",
    "            for i in range(1, 11):\n",
    "                kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "                kmeans.fit(self.X)\n",
    "                wcss.append(kmeans.inertia_)\n",
    "                print(f\"WCSS for n_clusters = {i}: {kmeans.inertia_}\")\n",
    "            print(\"WCSS computed successfully for all shortlisted number of clusters!\")\n",
    "            ################### Finalize dispersal point as the ideal number of clusters #####################\n",
    "            print(\"Finding the ideal number of clusters (by locating the dispersal point) via Elbow method..\")\n",
    "            knee_finder = KneeLocator(\n",
    "                range(1, 11), wcss, curve = 'convex', direction = 'decreasing')  # range(1, 11) vs WCSS\n",
    "            print(f\"Ideal number of clusters to be formed: {knee_finder.knee}\")\n",
    "            return knee_finder.knee\n",
    "            ...\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise e\n",
    "\n",
    "    def create_clusters(self) -> Tuple:\n",
    "        \"\"\"Divides the given data instances into the different clusters, they first hand shoud've been divided into\n",
    "        via offcourse Kmeans Clustering algorithm.\n",
    "        Raises:\n",
    "            e: Raises relevant exception should any kinda error pops up while dividing the given data instances into\n",
    "            clusters.\n",
    "        Returns:\n",
    "            (KMeans, np.array): KMeans Clustering object being used to cluster the given data instances and the given dataset \n",
    "            along with the cluster labels, respectively.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            ideal_clusters = self._get_ideal_number_of_clusters()\n",
    "            print(f\"Dividing the \\\"{self.desc}\\\" instances into {ideal_clusters} clusters via KMeans Clustering algorithm..\")\n",
    "            kmeans = KMeans(n_clusters=ideal_clusters, init = 'k-means++', random_state = 42)\n",
    "            y_kmeans = kmeans.fit_predict(self.X)\n",
    "            print(f\"..said data instances divided into {ideal_clusters} clusters successfully!\")\n",
    "            return kmeans, np.c_[self.X, y_kmeans]\n",
    "            ...\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6380cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster `Wafer` instances\n",
    "cluster_wafers = ClusterDataInstances(X = X_trans, desc = \"wafers features\")\n",
    "clusterer, X_clus = cluster_wafers.create_clusters()\n",
    "X_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusters\n",
    "np.unique(X_clus[ :, -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d3714d7",
   "metadata": {},
   "source": [
    "**=>** So the dataset was divided into 3 optimal clusters.\n",
    "\n",
    "Let's have a look at their shapes.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01f9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configure \"Clustered\" array along with target features\n",
    "wafers_clus = np.c_[X_clus, y]\n",
    "## Cluster_1 data\n",
    "wafers_1 = wafers_clus[wafers_clus[ :, -2] == 0]\n",
    "wafers_1.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "212a5b9e",
   "metadata": {},
   "source": [
    "**=>** Perhaps we were wrong about dividing the `Wafers` dataset into clusters, as we can see pretty much of all datapoints lie in the first cluster itself.\n",
    "\n",
    "Let's take look at another clusters anyway.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db45a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster_2 data\n",
    "wafers_clus[wafers_clus[ :, -2] == 1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f16f749",
   "metadata": {},
   "source": [
    "**=>** Man, seriously?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d0f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster_3 data\n",
    "wafers_clus[wafers_clus[ :, -2] == 2].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b308ebe8",
   "metadata": {},
   "source": [
    "**=>** Thus we mustn't divide the dataset into clusters. Not a good idea!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fffff94d",
   "metadata": {},
   "source": [
    "## # Resampling of Training Instances:\n",
    "\n",
    "Resampling is imperative in this case as the target variable is highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ac3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "X, y = X_trans[ :, :-1], y\n",
    "resampler = SMOTETomek(sampling_strategy = \"auto\")\n",
    "X_res, y_res = resampler.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73136d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before resampling, Shape of training instances: \", np.c_[X, y].shape)\n",
    "print(\"After resampling, Shape of training instances: \", np.c_[X_res, y_res].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac3e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target Cats after Resampling\n",
    "print(np.unique(y_res))\n",
    "print(f\"Value Counts: \\n-1: {len(y_res[y_res == -1])}, 1: {len(y_res[y_res == 1])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "161994a6",
   "metadata": {},
   "source": [
    "**=>** Exactly what we wanted!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed2eb8ab",
   "metadata": {},
   "source": [
    "### # Prepare the Test set:\n",
    "\n",
    "Do exactly the same to test set whatever's been done to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0203476",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size = 1/3, random_state = 42)\n",
    "\n",
    "print(f\"train set: {X_train.shape, y_train.shape}\")\n",
    "print(f\"test set: {X_test.shape, y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1945ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fetch only features that were used in training\n",
    "# X_test, y_test = wafers_test[preprocessing_pipeline.feature_names_in_], wafers_test.iloc[:, -1]\n",
    "\n",
    "# ## Transform the Test features\n",
    "# X_test_trans = preprocessing_pipeline.transform(X_test)\n",
    "# print(X_test_trans.shape, y_test.shape)\n",
    "\n",
    "# ## Cluster the test features\n",
    "# y_test_kmeans = clusterer.predict(X_test_trans)\n",
    "\n",
    "# ## Configure the test array\n",
    "# test_arr = np.c_[X_test_trans, y_test, y_test_kmeans]\n",
    "# np.unique(y_test_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the test features and test labels for cluster one\n",
    "\n",
    "# X_test_prep, y_test_prep = test_arr[test_arr[:, -2] == ], test_arr[:, -1]\n",
    "# print(X_test_prep.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "471d878d",
   "metadata": {},
   "source": [
    "## # Model Selection and Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb27df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost==0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e670eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepared training sets\n",
    "# X_prep = wafers_1[:, :-2]\n",
    "# y_prep = wafers_1[:, -1]\n",
    "# print(X_prep.shape, y_prep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepared training and test sets\n",
    "X_prep = X_train\n",
    "y_prep = y_train\n",
    "X_test_prep = X_test\n",
    "y_test_prep = y_test\n",
    "\n",
    "print(X_prep.shape, y_prep.shape)\n",
    "print(X_test_prep.shape, y_test_prep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a4af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Shortlisted base Models\n",
    "svc_clf = SVC(kernel = 'linear')\n",
    "svc_rbf_clf = SVC(kernel = 'rbf')\n",
    "random_clf = RandomForestClassifier(random_state = 42)\n",
    "xgb_clf = XGBClassifier(objective = 'binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff943535",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to display Scores\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Standard Deviation: \", scores.std())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70869e21",
   "metadata": {},
   "source": [
    "### # Evaluating `SVC (kernel='linear')` using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aa8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVC Scores\n",
    "svc_scores = cross_val_score(svc_clf, X_prep, y_prep, scoring='roc_auc', cv=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2e0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(svc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90adbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance on test set using cross-validation\n",
    "\n",
    "# Predictions using cross-validation\n",
    "svc_preds = cross_val_predict(svc_clf, X_test_prep, y_test_prep, cv = 5)\n",
    "\n",
    "# AUC score\n",
    "svc_auc = roc_auc_score(y_test_prep, svc_preds)\n",
    "svc_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eeab8048",
   "metadata": {},
   "source": [
    "### # Evaluating `SVC (kernel='rbf')` using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cac157",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVC rbf Scores\n",
    "svc_rbf_scores = cross_val_score(svc_rbf_clf, X_prep, y_prep, scoring = 'roc_auc', cv = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa746622",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(svc_rbf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance on test set using cross-validation\n",
    "\n",
    "# Predictions using cross-validation\n",
    "svc_rbf_preds = cross_val_predict(svc_rbf_clf, X_test_prep, y_test_prep, cv = 5)\n",
    "\n",
    "# AUC score\n",
    "svc_rbf_auc = roc_auc_score(y_test_prep, svc_rbf_preds)\n",
    "svc_rbf_auc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e923031",
   "metadata": {},
   "source": [
    "### # Evaluating `RandomForestClassifier` using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b426ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest Scores\n",
    "random_clf_scores = cross_val_score(random_clf, X_prep, y_prep, scoring = 'roc_auc', cv = 10, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735500b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(random_clf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0461bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance on test set using cross-validation\n",
    "\n",
    "# Predictions using cross-validation\n",
    "random_clf_preds = cross_val_predict(random_clf, X_test_prep, y_test_prep, cv = 5)\n",
    "\n",
    "# AUC score\n",
    "random_clf_auc = roc_auc_score(y_test_prep, random_clf_preds)\n",
    "random_clf_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
